{"cells":[{"cell_type":"code","execution_count":1,"id":"Ggkit1054anS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20328,"status":"ok","timestamp":1733881624704,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"Ggkit1054anS","outputId":"4bc36370-7ed4-4e93-927d-dcf6af0fb4ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/Money Printer\n"]}],"source":["# Mount into drive\n","\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","%cd '/content/drive/MyDrive/Colab Notebooks/Money Printer/'\n","\n","# !pip install -r requirements.txt\n","# !pip install pandas\n","# !pip install -U scikit-learn\n","# !pip install torchinfo"]},{"cell_type":"code","execution_count":3,"id":"cbcd3206-b1f6-4fac-9e98-19ce741cbc84","metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1733881660496,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"cbcd3206-b1f6-4fac-9e98-19ce741cbc84"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from pathlib import Path"]},{"cell_type":"markdown","id":"s645jstoYHi-","metadata":{"id":"s645jstoYHi-"},"source":["# Data Loader Script"]},{"cell_type":"code","execution_count":4,"id":"Wu8de6e8X970","metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1733881662791,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"Wu8de6e8X970"},"outputs":[],"source":["def get_data_loaders(device, label=\"label_1\", batch_size=64, use_custom_cols=True, window_size=10, flatten=True):\n","    train_data = pd.read_csv(\"./data/Train_NoAuction_Zscore_test.csv\")\n","    test_data = pd.read_csv(\"./data/Test_NoAuction_Zscore_test.csv\")\n","\n","    X_train = train_data.drop(\n","        columns=[\"label_1\", \"label_2\", \"label_3\", \"label_5\", \"label_10\"])\n","    X_test = test_data.drop(\n","        columns=[\"label_1\", \"label_2\", \"label_3\", \"label_5\", \"label_10\"])\n","\n","    y_train = train_data[label] - 1\n","    y_test = test_data[label] - 1\n","    if not use_custom_cols:\n","        X_train = X_train.iloc[:, :40]\n","        X_test = X_test.iloc[:, :40]\n","\n","    X_train = X_train.to_numpy()\n","    X_test = X_test.to_numpy()\n","    y_train = y_train.to_numpy()\n","    y_test = y_test.to_numpy()\n","\n","    # sliding window\n","    if window_size != 1:\n","        D = X_train.shape[1]\n","        X_train = np.lib.stride_tricks.sliding_window_view(\n","            X_train, (window_size, D))\n","        if flatten:\n","          X_train = X_train.reshape((-1, window_size*D))\n","        y_train = y_train[window_size-1:]\n","\n","        X_test = np.lib.stride_tricks.sliding_window_view(\n","            X_test, (window_size, D)).squeeze()\n","        if flatten:\n","            X_test = X_test.reshape((-1, window_size*D))\n","        y_test = y_test[window_size-1:]\n","\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_train, y_train, shuffle=True, test_size=0.2)\n","\n","    X_train_tensor = torch.tensor(\n","        X_train, dtype=torch.float32).to(device)\n","    X_val_tensor = torch.tensor(\n","        X_val, dtype=torch.float32).to(device)\n","    # Unsqueeze here for DeepLOB and TransLOB models:\n","    X_test_tensor = torch.tensor(\n","        X_test, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n","    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n","\n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(\n","        val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False)\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","id":"8a7513c8-b81e-4a42-b324-223201a1d3a5","metadata":{"id":"8a7513c8-b81e-4a42-b324-223201a1d3a5"},"source":["# Model Script:"]},{"cell_type":"code","execution_count":5,"id":"33887e45-d446-40bb-b891-d14bfb47bd13","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1733881663655,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"33887e45-d446-40bb-b891-d14bfb47bd13"},"outputs":[],"source":["class MultiHeadSelfAttention(nn.Module):\n","    \"\"\"\n","    Multi-head Self-Attention layer in PyTorch.\n","    \"\"\"\n","    def __init__(self, d_model: int, num_heads: int, use_masking: bool = False):\n","        \"\"\"\n","        :param d_model: Dimensionality of the input features.\n","        :param num_heads: Number of attention heads.\n","        :param use_masking: Whether to apply causal masking for autoregressive tasks.\n","        \"\"\"\n","        super(MultiHeadSelfAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.use_masking = use_masking\n","        self.depth = d_model // num_heads\n","\n","        # Query, Key, and Value projections\n","        self.qkv_proj = nn.Linear(d_model, d_model * 3, bias=False)\n","\n","        # Output projection\n","        self.out_proj = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x: torch.Tensor):\n","        \"\"\"\n","        Forward pass for multi-head self-attention.\n","        :param x: Input tensor of shape (batch_size, seq_len, d_model).\n","        :return: Output tensor of shape (batch_size, seq_len, d_model).\n","        \"\"\"\n","        batch_size, seq_len, _ = x.size()\n","\n","        # Project inputs to Query, Key, and Value tensors\n","        qkv = self.qkv_proj(x)  # (batch_size, seq_len, d_model * 3)\n","        q, k, v = torch.chunk(qkv, chunks=3, dim=-1)\n","\n","        # Reshape for multi-head attention\n","        q = q.view(batch_size, seq_len, self.num_heads, self.depth).permute(0, 2, 1, 3)\n","        k = k.view(batch_size, seq_len, self.num_heads, self.depth).permute(0, 2, 3, 1)\n","        v = v.view(batch_size, seq_len, self.num_heads, self.depth).permute(0, 2, 1, 3)\n","\n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k) / np.sqrt(self.depth)  # (batch_size, num_heads, seq_len, seq_len)\n","\n","        if self.use_masking:\n","            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n","            scores = scores.masked_fill(mask, float('-inf'))\n","\n","        attention_weights = F.softmax(scores, dim=-1)\n","        attention = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n","\n","        # Reshape and combine heads\n","        attention = attention.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, depth)\n","        attention = attention.view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n","\n","        # Apply output projection\n","        output = self.out_proj(attention)\n","        return output"]},{"cell_type":"code","execution_count":6,"id":"7ef815e4-6ef7-4d54-9bc4-24f2a362356e","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733881664558,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"7ef815e4-6ef7-4d54-9bc4-24f2a362356e"},"outputs":[],"source":["class LayerNormalization(nn.Module):\n","    \"\"\"\n","    Implementation of Layer Normalization.\n","    \"\"\"\n","    def __init__(self, d_model: int, eps=1e-5):\n","        \"\"\"\n","        :param d_model: Dimensionality of the input.\n","        :param eps: A small epsilon to avoid division by zero.\n","        \"\"\"\n","        super(LayerNormalization, self).__init__()\n","        self.gain = nn.Parameter(torch.ones(d_model))\n","        self.bias = nn.Parameter(torch.zeros(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n","        normalized = (x - mean) / torch.sqrt(variance + self.eps)\n","        return self.gain * normalized + self.bias\n","\n","\n","class TransformerTransition(nn.Module):\n","    \"\"\"\n","    Transformer transition function with feed-forward layers.\n","    \"\"\"\n","    def __init__(self, d_model: int, size_multiplier: int = 4, activation=F.relu):\n","        \"\"\"\n","        :param d_model: Dimensionality of the input/output.\n","        :param size_multiplier: Multiplier for the hidden layer size.\n","        :param activation: Activation function.\n","        \"\"\"\n","        super(TransformerTransition, self).__init__()\n","        self.activation = activation\n","        self.hidden_layer = nn.Linear(d_model, size_multiplier * d_model)\n","        self.output_layer = nn.Linear(size_multiplier * d_model, d_model)\n","\n","    def forward(self, x):\n","        x = self.activation(self.hidden_layer(x))\n","        x = self.output_layer(x)\n","        return x\n","\n","\n","class TransformerBlock(nn.Module):\n","    \"\"\"\n","    A single Transformer block with self-attention, residual connections,\n","    normalization, and a feed-forward transition layer.\n","    \"\"\"\n","    def __init__(self, d_model: int, num_heads: int, use_masking: bool = True, size_multiplier: int = 4, dropout_rate: float = 0.1):\n","        \"\"\"\n","        :param d_model: Dimensionality of the model.\n","        :param num_heads: Number of attention heads.\n","        :param use_masking: Whether to apply masking in the attention mechanism.\n","        :param size_multiplier: Multiplier for the hidden size in the feed-forward layer.\n","        :param dropout_rate: Dropout probability.\n","        \"\"\"\n","        super(TransformerBlock, self).__init__()\n","        self.self_attention = MultiHeadSelfAttention(d_model, num_heads, use_masking)\n","        self.norm1 = LayerNormalization(d_model)\n","        self.norm2 = LayerNormalization(d_model)\n","        self.transition = TransformerTransition(d_model, size_multiplier)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        # Self-attention with residual connection and layer normalization\n","        attention_output = self.self_attention(x)\n","        attention_output = self.dropout(attention_output)\n","        x = self.norm1(x + attention_output)\n","\n","        # Feed-forward transition with residual connection and layer normalization\n","        transition_output = self.transition(x)\n","        transition_output = self.dropout(transition_output)\n","        x = self.norm2(x + transition_output)\n","\n","        return x"]},{"cell_type":"code","execution_count":7,"id":"cca42c43-b627-40ee-9526-1e092656032d","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1733881664840,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"cca42c43-b627-40ee-9526-1e092656032d"},"outputs":[],"source":["class CausalConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation):\n","        super(CausalConv1d, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.dilation = dilation\n","        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation)\n","\n","    def forward(self, x):\n","        # Calculate the padding for causal convolution with dilation\n","        padding = (self.kernel_size - 1) * self.dilation\n","        # Pad the input tensor on the left (only past context)\n","        x = F.pad(x, (padding, 0), mode='constant', value=0)\n","        # Perform the convolution\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":8,"id":"b0f31dc1-3578-4d55-8f71-b188ee946d5d","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1733881665126,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"b0f31dc1-3578-4d55-8f71-b188ee946d5d"},"outputs":[],"source":["class TransLOB(nn.Module):\n","    def __init__(self, n_classes=3):\n","        super(TransLOB, self).__init__()\n","\n","        self.n_classes = n_classes\n","        # 1. Convolutional layers\n","        ## 1st convolution:\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","        )\n","\n","        ## 2nd:\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n","            nn.Tanh(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.Tanh(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.Tanh(),\n","            nn.BatchNorm2d(32),\n","        )\n","\n","        ## 3rd:\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(32),\n","        )\n","\n","        # 2. Inception Layer:\n","        ##1st inc 1x1 3x1\n","        self.inc1 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 1), padding='same'),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 1), padding='same'),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(64),\n","        )\n","\n","        ##2nd inc 1x1 5x1\n","        self.inc2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 1), padding='same'),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 1), padding='same'),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(64),\n","        )\n","\n","        ##3nd inc max_pool 1x1\n","        self.inc3 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0)),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 1), padding='same'),\n","            nn.LeakyReLU(negative_slope=0.01),\n","            nn.BatchNorm2d(64),\n","        )\n","\n","        #Dilated CNN:\n","        self.dilated = nn.Sequential(\n","            CausalConv1d(192, 14, kernel_size=2, stride=1, dilation=1),\n","            nn.ReLU(),\n","            CausalConv1d(14, 14, kernel_size=2, stride=1, dilation=2),\n","            nn.ReLU(),\n","            CausalConv1d(14, 14, kernel_size=2, stride=1, dilation=4),\n","            nn.ReLU(),\n","            CausalConv1d(14, 14, kernel_size=2, stride=1, dilation=8),\n","            nn.ReLU(),\n","            CausalConv1d(14, 14, kernel_size=2, stride=1, dilation=16),\n","            nn.ReLU(),\n","        )\n","\n","        #Layer Normalization block:\n","        # self.norm1 = nn.LayerNorm()\n","\n","        #Transformer block\n","        self.transformer1 = TransformerBlock(d_model=15, num_heads=3)\n","        self.transformer2 = TransformerBlock(d_model=15, num_heads=3)\n","\n","        #Feed forward block (MLP)\n","        self.fc1 = nn.Linear(15*82,64)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(64, self.n_classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def positional_encoding(self, x):\n","        \"\"\"\n","        Adds positional encoding to the input tensor in PyTorch.\n","        :param x: Input tensor of shape (batch_size, steps, d_model)\n","        :return: Tensor with added positional encoding (batch_size, steps, d_model+1)\n","        \"\"\"\n","        # Extract sequence length and model dimension\n","        steps, d_model = x.shape[1], x.shape[2]\n","\n","        # Compute positional encoding\n","        ps = torch.linspace(-1, 1, steps, dtype=x.dtype, device=x.device).view(-1, 1)  # Shape: (steps, 1)\n","        ps = ps.unsqueeze(0).expand(x.size(0), -1, -1)  # Expand to batch size, Shape: (batch_size, steps, 1)\n","\n","        # Concatenate positional encoding to the input\n","        x = torch.cat([x, ps], dim=-1)  # Shape: (batch_size, steps, d_model + 1)\n","\n","        return x\n","\n","    def forward(self, x):\n","        #Normal CNN\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","\n","        #Inception:\n","        inception1 = self.inc1(x)\n","        inception2 = self.inc2(x)\n","        inception3 = self.inc3(x)\n","        x = torch.cat((inception1, inception2, inception3), dim=1)\n","\n","        # Reshape for Dilated CNN (from 4D to 3D)\n","        batch, channels, height, width = x.size()\n","        x = x.view(batch, channels, -1)\n","\n","\n","        #Dilated CNN:\n","        x = self.dilated(x)\n","        # x = self.norm1(x)\n","\n","\n","        #Positional encoding:\n","        x = x.permute(0, 2, 1)\n","        x = self.positional_encoding(x)\n","        # print(x.shape)\n","\n","        #Transformer block:\n","        x = self.transformer1(x)\n","        x = self.transformer2(x)\n","\n","        #MLP and output:\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout(self.fc1(x))\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        output = self.softmax(x)\n","\n","        return output"]},{"cell_type":"markdown","id":"d07cf340-c1c3-4197-9b3e-38da855ba503","metadata":{"id":"d07cf340-c1c3-4197-9b3e-38da855ba503"},"source":["# Training and Testing Script"]},{"cell_type":"code","execution_count":9,"id":"QbmFXc9sN9iA","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733881665671,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"QbmFXc9sN9iA"},"outputs":[],"source":["def test_model(model, test_loader, experiment_name, save=True, display=False):\n","    test_predictions = []\n","    test_labels = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch, (X, y) in enumerate(test_loader):\n","          pred = model(X)\n","          _, predicted = torch.max(pred, 1)\n","          test_predictions.extend(predicted.cpu().numpy())\n","          test_labels.extend(y.cpu().numpy())\n","    report = classification_report(test_labels, test_predictions)\n","    if display:\n","        print(report)\n","    if save:\n","        with open(f\"results/{experiment_name}/report.txt\", \"w\") as f:\n","            f.write(report)\n","\n","\n","def visualize_curves(train_losses, val_losses, train_accuracies, val_accuracies, experiment_name, save=True, display=False):\n","    plt.plot(np.arange(epochs), train_losses, label=\"train\")\n","    plt.plot(np.arange(epochs), val_losses, label=\"val\")\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.title(f\"{experiment_name} loss curves\")\n","    plt.legend()\n","    if display:\n","        plt.show()\n","    if save:\n","        plt.savefig(f\"results/{experiment_name}/loss_curves.png\")\n","    plt.close()\n","\n","    plt.plot(np.arange(epochs), train_accuracies, label=\"train\")\n","    plt.plot(np.arange(epochs), val_accuracies, label=\"val\")\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"accuracy\")\n","    plt.title(f\"{experiment_name} accuracy curves\")\n","    plt.legend()\n","    if display:\n","        plt.show()\n","    if save:\n","        plt.savefig(f\"results/{experiment_name}/accuracy_curves.png\")\n","    plt.close()\n","\n","\n","def train_and_evaluate_model(train_loader, val_loader, test_loader, experiment_name, optimizer, criterion, epochs):\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for i in range(epochs):\n","        model.train()\n","        running_train_loss = 0\n","        train_correct_predictions = 0\n","        train_total_samples = 0\n","        for batch, (X, y) in enumerate(train_loader):\n","            pred = model(X)\n","            loss = criterion(pred, y)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            running_train_loss += loss.item()\n","\n","            _, predicted = torch.max(pred, 1)\n","            train_correct_predictions += (predicted == y).sum().item()\n","            train_total_samples += y.size(0)\n","\n","        print(f\"Epoch {i} train loss {running_train_loss/len(train_loader)}\")\n","        train_losses.append(running_train_loss/len(train_loader))\n","\n","        train_accuracy = train_correct_predictions / train_total_samples\n","        train_accuracies.append(train_accuracy)\n","        print(f\"Epoch {i} train accuracy {train_accuracy}\")\n","\n","        val_correct_predictions = 0\n","        val_total_samples = 0\n","        model.eval()\n","        with torch.no_grad():\n","            running_val_loss = 0\n","            for batch, (X, y) in enumerate(val_loader):\n","                pred = model(X)\n","                loss = criterion(pred, y)\n","                running_val_loss += loss.item()\n","\n","                _, predicted = torch.max(pred, 1)\n","                val_correct_predictions += (predicted == y).sum().item()\n","                val_total_samples += y.size(0)\n","\n","            print(f\"Epoch {i} val loss {running_val_loss/len(val_loader)}\")\n","            val_losses.append(running_val_loss/len(val_loader))\n","\n","            val_accuracy = val_correct_predictions / val_total_samples\n","            val_accuracies.append(val_accuracy)\n","            print(f\"Epoch {i} val accuracy {val_accuracy}\")\n","\n","    test_model(model, test_loader, experiment_name)\n","    visualize_curves(train_losses, val_losses,\n","                     train_accuracies, val_accuracies, experiment_name)"]},{"cell_type":"code","execution_count":10,"id":"c_J2e2Y-YWDV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268866,"status":"ok","timestamp":1733881935861,"user":{"displayName":"Loc Nguyen","userId":"14674290205867162983"},"user_tz":360},"id":"c_J2e2Y-YWDV","outputId":"a9b8851c-513c-4dbf-a54f-79e852ebb607"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n","Epoch 0 train loss 0.9520773955553126\n","Epoch 0 train accuracy 0.5939498278406297\n","Epoch 0 val loss 1.0297817424871027\n","Epoch 0 val accuracy 0.40304930529939753\n","Epoch 1 train loss 0.9456025285898819\n","Epoch 1 train accuracy 0.5962862764387604\n","Epoch 1 val loss 0.960586444940418\n","Epoch 1 val accuracy 0.5855157998278618\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 train loss 0.9461910195341279\n","Epoch 0 train accuracy 0.5943187407771766\n","Epoch 0 val loss 1.0011821035295725\n","Epoch 0 val accuracy 0.5911717693348087\n","Epoch 1 train loss 0.9449394047845792\n","Epoch 1 train accuracy 0.5938883423512051\n","Epoch 1 val loss 0.9729921412654221\n","Epoch 1 val accuracy 0.5911717693348087\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 train loss 0.9454477850955221\n","Epoch 0 train accuracy 0.5945031972454501\n","Epoch 0 val loss 0.9809569735080004\n","Epoch 0 val accuracy 0.5976884298536825\n","Epoch 1 train loss 0.9476392106363488\n","Epoch 1 train accuracy 0.5925664043285784\n","Epoch 1 val loss 0.9551817998290062\n","Epoch 1 val accuracy 0.5976884298536825\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 train loss 0.9457248271331338\n","Epoch 0 train accuracy 0.5929660600098376\n","Epoch 0 val loss 0.9425955102778971\n","Epoch 0 val accuracy 0.6043280462314029\n","Epoch 1 train loss 0.943529123875856\n","Epoch 1 train accuracy 0.5949028529267093\n","Epoch 1 val loss 1.1585507094860077\n","Epoch 1 val accuracy 0.19881962375507192\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 train loss 0.9419318106179153\n","Epoch 0 train accuracy 0.5968703885882931\n","Epoch 0 val loss 0.9445130885578692\n","Epoch 0 val accuracy 0.5959670478298291\n","Epoch 1 train loss 0.9430100647311782\n","Epoch 1 train accuracy 0.5946876537137236\n","Epoch 1 val loss 1.0299532958306372\n","Epoch 1 val accuracy 0.4817410549612689\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["# get device\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")\n","\n","# hyperparameters\n","learning_rate = 5e-4\n","batch_size = 64\n","epochs = 50\n","window_size = 100\n","\n","# model definition\n","# model = MLP(input_size=NUM_FEATURES*window_size, hidden_size=64)\n","model = TransLOB(n_classes = 3)\n","model = model.to(device)\n","\n","# experiment name\n","experiment_name = \"translob-s10\"\n","\n","# loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","for label in [\"label_1\", \"label_2\", \"label_3\", \"label_5\", \"label_10\"]:\n","    new_experiment_name = experiment_name + f\"/{label}\"\n","    Path(\n","        f\"results/{new_experiment_name}\").mkdir(parents=True, exist_ok=True)\n","\n","    # get data loaders\n","    train_loader, val_loader, test_loader = get_data_loaders(device=device, window_size=100, batch_size=64,\n","                                                              flatten=False,use_custom_cols=False)\n","\n","    # training and evaluation\n","    train_and_evaluate_model(train_loader, val_loader,\n","                              test_loader, new_experiment_name, optimizer, criterion, epochs)"]},{"cell_type":"code","execution_count":null,"id":"KScFJOcwZF1d","metadata":{"id":"KScFJOcwZF1d"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"}},"nbformat":4,"nbformat_minor":5}
